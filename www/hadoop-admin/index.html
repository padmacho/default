<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="/style.css" type="text/css" />
</head>
<body>
<h1 id="big-data-hadoop">Big Data &amp; Hadoop</h1>
<h2 id="about">About</h2>
<p>Hadoop Development Training course teaches experienced / knowledge peoples on purpose of Hadoop Technology, how to setup Hadoop Cluster, how to store BigData using Hadoop (HDFS) and how to process/analyze the BigData using Map-Reduce Programming or by using other Hadoop ecosystems.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Basic Linux Commands</li>
<li>Core Java (OOPS Concepts, Collections , Exceptions ) — For Map-Reduce Programming</li>
<li>SQL Query knowledge – For Hive Queries</li>
</ul>
<h2 id="hardware-and-software-requirements">Hardware and Software Requirements</h2>
<ul>
<li>Virtual Box 5.1.14</li>
<li>Host operating system can be Windows 10 /Mac/ Ubuntu 16.10 (Preferable)</li>
<li>Java 1.8</li>
<li>Eclipse IDE Neon.2 Release (4.6.2)</li>
<li>Vagrant 1.9.2</li>
</ul>
<h2 id="fundamentals-of-hadoop">Fundamentals of Hadoop</h2>
<h3 id="introduction-to-big-data">Introduction to Big Data</h3>
<ul>
<li>What is Big data</li>
<li>Big Data opportunities</li>
<li>Big Data Challenges</li>
<li>Characteristics of Big data</li>
</ul>
<h3 id="introduction-to-hadoop">Introduction to Hadoop</h3>
<ul>
<li>High Availability</li>
<li>Scaling</li>
<li>Advantages and Challenges</li>
<li>MapReduce</li>
<li>HDFS (Hadoop Distributed File System)</li>
<li>YARN</li>
<li>I/O</li>
</ul>
<h3 id="installation">Installation</h3>
<ul>
<li>Standalone Mode</li>
<li>Pseudodistributed Mode</li>
<li>Fully Distributed Mode (Cluster)</li>
</ul>
<h2 id="mapreduce-development">MapReduce Development</h2>
<h3 id="developing-a-mapreduce-application">Developing a MapReduce Application</h3>
<ul>
<li>Configuration API</li>
<li>Setting Up the Development Environment</li>
<li>Unit Test with MRUnit</li>
<li>Running Locally and on Cluster</li>
<li>Tuning a Job</li>
</ul>
<h3 id="mapreduce-workflow">MapReduce Workflow</h3>
<ul>
<li>Decomposing a problem</li>
<li>JobControl</li>
<li>Oozie</li>
</ul>
<h3 id="mapreduce-typeformats-and-features">MapReduce Type,Formats and Features</h3>
<ul>
<li>Types</li>
<li>Input Formats</li>
<li>Output Formats</li>
</ul>
<h2 id="hadoop-administration">Hadoop Administration</h2>
<h3 id="setting-up-a-hadoop-cluster">Setting up a Hadoop Cluster</h3>
<ul>
<li>Specification</li>
<li>Sizing</li>
<li>Network Topology</li>
</ul>
<h3 id="security">Security</h3>
<ul>
<li>Kerberos and Hadoop</li>
<li>Tokens</li>
</ul>
<h3 id="benchmarking">Benchmarking</h3>
<ul>
<li>Benchmarking and User Jobs</li>
</ul>
<h3 id="hdfs">HDFS</h3>
<ul>
<li>Persistence Data Structures</li>
<li>Safe Mode</li>
<li>Auto Logging</li>
</ul>
<h3 id="monitoring">Monitoring</h3>
<ul>
<li>Logging</li>
<li>Metrics and JMX</li>
</ul>
<h3 id="maintenance">Maintenance</h3>
<ul>
<li>Routine Administration Procedures</li>
<li>Commissioning and Decommissioning Nodes</li>
</ul>
<h2 id="hadoop-ecosystem">Hadoop Ecosystem</h2>
<h3 id="nosql">NoSQL</h3>
<ul>
<li>ACID in RDBMS and BASE in NoSQL.</li>
<li>CAP Theorem and Types of Consistency.</li>
<li>Types of NoSQL Databases in detail.</li>
<li>Columnar Databases in Detail (HBASE and CASSANDRA).</li>
<li>TTL, Bloom Filters and Compensation.</li>
</ul>
<h3 id="hbase">HBase</h3>
<ul>
<li>HBase Installation</li>
<li>HBase concepts</li>
<li>HBase Data Model and Comparison between RDBMS and NOSQL.</li>
<li>Master &amp; Region Servers.</li>
<li>HBase Operations (DDL and DML) through Shell and Programming and HBase Architecture.</li>
<li>Catalog Tables.</li>
<li>Block Cache and sharding.</li>
<li>SPLITS.</li>
<li>DATA Modeling (Sequential, Salted, Promoted and Random Keys).</li>
<li>JAVA API’s and Rest Interface.</li>
<li>Client Side Buffering and Process 1 million records using Client side Buffering.</li>
<li>HBASE Counters.</li>
<li>Enabling Replication and HBASE RAW Scans.</li>
<li>HBASE Filters.</li>
<li>Bulk Loading and Coprocessors (Endpoints and Observers with programs).</li>
<li>Real world use case consisting of HDFS,MR and HBASE.</li>
</ul>
<h3 id="pig">Pig</h3>
<ul>
<li>Installation</li>
<li>Execution Types</li>
<li>Grunt Shell</li>
<li>Pig Latin</li>
<li>Data Processing</li>
<li>Schema on read</li>
<li>Primitive data types and complex data types.</li>
<li>Tuple schema, BAG Schema and MAP Schema.</li>
<li>Loading and Storing</li>
<li>Filtering</li>
<li>Grouping &amp; Joining</li>
<li>Debugging commands (Illustrate and Explain).</li>
<li>Validations in PIG.</li>
<li>Type casting in PIG.</li>
<li>Working with Functions</li>
<li>User Defined Functions</li>
<li>Types of JOINS in pig and Replicated Join in detail.</li>
<li>SPLITS and Multiquery execution.</li>
<li>Error Handling, FLATTEN and ORDER BY.</li>
<li>Parameter Substitution.</li>
<li>Nested For Each.</li>
<li>User Defined Functions, Dynamic Invokers and Macros.</li>
<li>How to access HBASE using PIG.</li>
<li>How to Load and Write JSON DATA using PIG.</li>
<li>Piggy Bank.</li>
<li>Hands on Exercises</li>
</ul>
<h3 id="hive">Hive</h3>
<ul>
<li>Installation</li>
<li>Introduction and Architecture.</li>
<li>Hive Services, Hive Shell, Hive Server and Hive Web Interface (HWI)</li>
<li>Meta store</li>
<li>Hive QL</li>
<li>OLTP vs. OLAP</li>
<li>Working with Tables.</li>
<li>Primitive data types and complex data types.</li>
<li>Working with Partitions.</li>
<li>User Defined Functions</li>
<li>Hive Bucketed Tables and Sampling.</li>
<li>External partitioned tables, Map the data to the partition in the table, Writing the output of one query to another table, Multiple inserts</li>
<li>Dynamic Partition</li>
<li>Differences between ORDER BY, DISTRIBUTE BY and SORT BY.</li>
<li>Bucketing and Sorted Bucketing with Dynamic partition.</li>
<li>RC File.</li>
<li>INDEXES and VIEWS.</li>
<li>MAPSIDE JOINS.</li>
<li>Compression on hive tables and Migrating Hive tables.</li>
<li>Dynamic substation of Hive and Different ways of running Hive</li>
<li>How to enable Update in HIVE.</li>
<li>Log Analysis on Hive.</li>
<li>Access HBASE tables using Hive.</li>
<li>Hands on Exercises</li>
</ul>
<h3 id="flume">Flume</h3>
<ul>
<li>Installation</li>
<li>Introduction to Flume</li>
<li>Flume Agents: Sources, Channels and Sinks</li>
<li>Log User information using Java program in to HDFS using LOG4J and Avro Source</li>
<li>Log User information using Java program in to HDFS using Tail Source</li>
<li>Log User information using Java program in to HBASE using LOG4J and Avro Source</li>
<li>Log User information using Java program in to HBASE using Tail Source</li>
<li>Flume Commands</li>
<li>Use case of Flume: Flume the data from twitter in to HDFS and HBASE. Do some analysis using HIVE and PIG</li>
</ul>
<h3 id="sqoop">Sqoop</h3>
<ul>
<li>Installation</li>
<li>Import Data.(Full table, Only Subset, Target Directory, protecting Password, file format other than CSV,Compressing,Control Parallelism, All tables Import)<br />
Incremental Import(Import only New data, Last Imported data, storing Password in Metastore, Sharing Metastore between Sqoop Clients)</li>
<li>Free Form Query Import</li>
<li>Export data to RDBMS,HIVE and HBASE</li>
<li>Hands on Exercises.</li>
</ul>
<h3 id="spark">Spark</h3>
<ul>
<li>Overview</li>
<li>Linking with Spark</li>
<li>Initializing Spark</li>
<li>Using the Shell</li>
<li>Resilient Distributed Datasets (RDDs)</li>
<li>Parallelized Collections</li>
<li>External Datasets</li>
<li>RDD Operations</li>
<li>Basics, Passing Functions to Spark</li>
<li>Working with Key-Value Pairs</li>
<li>Transformations</li>
<li>Actions</li>
<li>RDD Persistence</li>
<li>Which Storage Level to Choose?</li>
<li>Removing Data</li>
<li>Shared Variables</li>
<li>Broadcast Variables</li>
<li>Accumulators</li>
<li>Deploying to a Cluster</li>
<li>Unit Testing</li>
</ul>
<h3 id="oozie">Oozie</h3>
<ul>
<li>Workflow (Action, Start, Action, End, Kill, Join and Fork), Schedulers, Coordinators and Bundles.</li>
<li>Workflow to show how to schedule Sqoop Job, Hive, MR and PIG.</li>
<li>Real world Use case which will find the top websites used by users of certain ages and will be scheduled to run for every one hour.</li>
<li>Zoo Keeper</li>
<li>HBASE Integration with HIVE and PIG.</li>
<li>Phoenix</li>
<li>Proof of concept (POC).</li>
</ul>
<h3 id="zookeeper">Zookeeper</h3>
<ul>
<li>Installing</li>
<li>Running</li>
<li>Zookeeper as service</li>
<li>Example</li>
<li>Building Application with Zookeeper</li>
</ul>
</body>
</html>
